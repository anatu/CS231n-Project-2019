{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBaseline with RBF kernel C-support SVM\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Baseline with RBF kernel C-support SVM\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from linear_classifier import *\n",
    "from linear_svm import *\n",
    "import time\n",
    "import os\n",
    "import numpy as np \n",
    "import json\n",
    "import imghdr\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.feature import hog, daisy\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image( infilename ) :\n",
    "    '''\n",
    "    Helper function to manually load in images. Accepts image path infilename \n",
    "    and returns the numpy array representation of the image data\n",
    "    (Need final X to be of dims (N, H, W, D))\n",
    "    '''\n",
    "    img = Image.open( infilename )\n",
    "    img.load()\n",
    "    data = np.asarray( img, dtype=\"int32\" )\n",
    "    return data\n",
    "\n",
    "def loadData(rootDir, featureType):\n",
    "    '''\n",
    "    Helper function which goes into one of our split dataset folders (train, val, test) to extract\n",
    "    image data. Loads images, performs custom feature extraction, extracts class label from file name,\n",
    "    and returns two numpy arrays X and y with data / labels for all examples in rootDir\n",
    "    '''\n",
    "    featList = []\n",
    "    labelList = []\n",
    "    imgList = []\n",
    "    ppc = 8\n",
    "    for c in os.listdir(rootDir):\n",
    "        for file in os.listdir(os.path.join(rootDir, c)):\n",
    "            filePath = os.path.join(rootDir, c, file)\n",
    "            if imghdr.what(filePath):\n",
    "                imRaw = load_image(filePath)\n",
    "                if featureType == \"hog\":\n",
    "                    feature = hog(imRaw, orientations=8, pixels_per_cell=(ppc,ppc), block_norm = \"L2\",\n",
    "                                    feature_vector = True, visualize = False, cells_per_block=(4,4)) # HOG Features\n",
    "                elif featureType == \"daisy\":\n",
    "                    imGray = color.rgb2gray(imRaw)\n",
    "                    feature = daisy(imGray, step=4, radius=15, rings=3, histograms=8, orientations=8,\n",
    "                                    normalization=\"l1\", visualize = False) # DAISY Features (similar to SIFT)\n",
    "                # Note: dataset-split/val\\compost\\compost168.jpg may not be resized yet!\n",
    "                else:\n",
    "                    print(\"Invalid custom feature type specified\")\n",
    "                    exit(0)\n",
    "                if imRaw.shape != (384, 512, 3):\n",
    "                    print(\"Skipping dimension-incompatible image {}\".format(file))\n",
    "                    continue\n",
    "                featList.append(feature)\n",
    "                labelList.append(c)\n",
    "                \n",
    "    X = np.stack(tuple(featList))\n",
    "    y = np.array(labelList)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Load in the training and validation data and perform HOG feature extraction.\n",
    "'''\n",
    "X_train, y_train = loadData(\"../../datasets/trashnet/data/dataset-split/train\", \"hog\")\n",
    "X_val, y_val = loadData(\"../../datasets/trashnet/data/dataset-split/val\", \"hog\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1574, 351360)\n",
      "(1574,)\n",
      "(788, 351360)\n",
      "(788,)\n"
     ]
    }
   ],
   "source": [
    "# Sanity check dimensions\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean image\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "X_train = X_train.astype(float) - mean_image\n",
    "X_val = X_val.astype(float) - mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the bias dimension of ones (i.e. bias trick) so that our SVM\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the class labels\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "classes = [\"cardboard\", \"compost\", \"glass\", \"metal\", \"paper\", \"plastic\", \"trash\"]\n",
    "le = LabelEncoder()\n",
    "le.fit(classes)\n",
    "y_train = le.transform(y_train)\n",
    "y_val = le.transform(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    }
   ],
   "source": [
    "# Train the SVC model\n",
    "clf = SVC(kernel=\"rbf\", gamma=0.5, verbose=True, C = 0.1)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline validation set accuracy is 0.215736\n"
     ]
    }
   ],
   "source": [
    "# Evaluate validation set acc\n",
    "y_val_pred = clf.predict(X_val)\n",
    "val_accuracy = np.mean(y_val_pred == y_val)\n",
    "print('Baseline validation set accuracy is %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline train set accuracy is 0.998094\n"
     ]
    }
   ],
   "source": [
    "# Training set accuracy\n",
    "y_train_pred = clf.predict(X_train)\n",
    "train_accuracy = np.mean(y_train == y_train_pred)\n",
    "print('Baseline train set accuracy is %f' % train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
